{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
        "                                 langchain-google-vertexai \\\n",
        "                                 langchain pypdf faiss-gpu chromadb"
      ],
      "metadata": {
        "id": "jNKWcsguX_dN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714222485549,
          "user_tz": -120,
          "elapsed": 43536,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf7b476-f735-481d-a55e-8ddf1fe953dd"
      },
      "id": "jNKWcsguX_dN",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "MO1xVdIsYNAK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917837,
          "user_tz": -120,
          "elapsed": 21,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "MO1xVdIsYNAK",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "YUEa41z2V9zbnYCuoT2lMj30",
      "metadata": {
        "tags": [],
        "id": "YUEa41z2V9zbnYCuoT2lMj30",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917837,
          "user_tz": -120,
          "elapsed": 21,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"crime-information-421611\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_google_vertexai import ChatVertexAI, HarmBlockThreshold, HarmCategory\n",
        "from vertexai.generative_models import Content, GenerativeModel, Part\n",
        "from pypdf import PdfReader\n",
        "\n",
        "import os\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import VertexAIEmbeddings"
      ],
      "metadata": {
        "id": "NmTs9NPgYQC_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917839,
          "user_tz": -120,
          "elapsed": 21,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "NmTs9NPgYQC_",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GenerativeModel(\"gemini-1.0-pro\")"
      ],
      "metadata": {
        "id": "lLbLjd-IYVbQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917839,
          "user_tz": -120,
          "elapsed": 20,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "lLbLjd-IYVbQ",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_docs = [\n",
        "    \"/content/2022-2023-Annual-Crime-Statistics-Report.pdf\",\n",
        "    \"/content/2023-2024 _Annual_WEB.pdf\",\n",
        "    \"/content/2023-2024_-_2nd_Quarter_WEB.pdf\",\n",
        "    \"/content/2023-2024_-_3nd_Quarter_WEB.pdf\",\n",
        "    \"/content/april_june_2023_24_quarter1_presentation.pdf\"\n",
        "]\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "G-nI3QagcToe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917839,
          "user_tz": -120,
          "elapsed": 19,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "G-nI3QagcToe",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        chunk_size=10000,\n",
        "        chunk_overlap=1000\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "b1GSmHoIdNBM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917840,
          "user_tz": -120,
          "elapsed": 19,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "b1GSmHoIdNBM",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector_store(text_chunks):\n",
        "    embeddings = VertexAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = Chroma.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"Chroma_index\")"
      ],
      "metadata": {
        "id": "MJkPG12yeAuu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917840,
          "user_tz": -120,
          "elapsed": 18,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "MJkPG12yeAuu",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conversation_chain():\n",
        "    prompt_template = \"\"\"\n",
        "        Answer the question clear and precise. If not provided the context return the result as\n",
        "        \"Sorry I dont know the answer\", don't provide the wrong answer.\n",
        "        Context:\\n {context}?\\n\n",
        "        Question:\\n{question}\\n\n",
        "        Answer:\n",
        "    \"\"\"\n",
        "    model = ChatVertexAI(\n",
        "      model_name=\"gemini-1.0-pro\",\n",
        "      safety_settings={\n",
        "          HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE\n",
        "      },\n",
        "    )\n",
        "    prompt = ChatPromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
        "    memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
        "    conversation = ConversationChain(llm=model, prompt=prompt, verbose=True, memory=memory)\n",
        "\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "Jpp1mLaDeUm-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714221917841,
          "user_tz": -120,
          "elapsed": 18,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "Jpp1mLaDeUm-",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_input(query, k=2):\n",
        "    file = \"/content/2022-2023-Annual-Crime-Statistics-Report.pdf\"\n",
        "    loader = PyPDFLoader(file)\n",
        "    documents = loader.load()\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    embeddings = VertexAIEmbeddings(model='models/embedding-001')\n",
        "    db = Chroma.from_documents(texts, embeddings)\n",
        "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
        "    # new_db = Chroma.load_local(\"Chroma_index\", embeddings)\n",
        "    # docs = new_db.similarity_search(user_question)\n",
        "\n",
        "    chain = get_conversation_chain()\n",
        "    response = chain.invoke(\n",
        "        {\"input_documents\": retriever, \"question\": query},\n",
        "        return_only_outputs=True\n",
        "    )\n",
        "\n",
        "    return response[\"output_text\"]"
      ],
      "metadata": {
        "id": "VtWTd5zPgu_y",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1714222683795,
          "user_tz": -120,
          "elapsed": 1410,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "VtWTd5zPgu_y",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_input(\"What is the crime rate in the city of Sandton?\"))"
      ],
      "metadata": {
        "id": "cuxg-sZthI4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1714222699359,
          "user_tz": -120,
          "elapsed": 10927,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "70d5956a-a614-4b6f-a718-c48476d275a2"
      },
      "id": "cuxg-sZthI4b",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.embeddings.vertexai:Model_name will become a required arg for VertexAIEmbeddings starting from Feb-01-2024. Currently the default is set to textembedding-gecko@001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'messages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-7a1f2c9c094d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the crime rate in the city of Sandton?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-5e12a1fab87f>\u001b[0m in \u001b[0;36muser_input\u001b[0;34m(query, k)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# docs = new_db.similarity_search(user_question)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_conversation_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     response = chain.invoke(\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"input_documents\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-394f4fcda121>\u001b[0m in \u001b[0;36mget_conversation_chain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m       },\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversationBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"history\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_messages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mconversation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversationChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lc_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/prompts/chat.py\u001b[0m in \u001b[0;36mvalidate_input_variables\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mValidated\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \"\"\"\n\u001b[0;32m--> 853\u001b[0;31m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         \u001b[0minput_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0minput_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'messages'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "aaishabintemuhammad (Apr 27, 2024, 1:44:57 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}